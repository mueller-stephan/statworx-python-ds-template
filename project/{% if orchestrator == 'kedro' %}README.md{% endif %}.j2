# {{ project_name.title() }}
------------------------------

{{ project_description }}

## Overview

**Client:** {{ client_name }}

**Project duration:**

**Status:**

## Contact

| Name            | Mail                         | Role                  |
| --------------- | ---------------------------- | --------------------- |
| {{ author_fullname }} | {{ author_email }} | Data Scientist          |



## Installation

The dependencies are managed using `poetry`.

```console
make install
```

## Usage

### Run Pipeline

You can run the entire end-to-end pipeline that processes the data, trains the model, and makes predictions using one of the two following commands. The `run_local` target runs the local pipeline with local data in the `data` folder. The `run_prod` target loads the data from snowflake and stores the predictions back into snowflake.

```console
make run_local  # local pipeline
make run_prod  # production pipeline
```

### Track Models with Mlflow

The model training is tracked with mlflow. The dashboard can be started by running the following command. After that it can be accessed on `http://localhost:5001`.

```console
make mlfow
```

### Test Source Code

Have a look at the file `src/tests/test_run.py` for instructions on how to write your tests. You can run your tests as follows:

```console
make test
```

### Vizulize Pipeline Graph

Kedro provides a user interface called `kedro-viz` that visualizes the implemented pipelines and nodes.
The dashboard can be started by running the following command. After that it can be accessed on `http://localhost:4141`.


```console
make viz
```

### Build Documentation

Your can build the documentation using `mkdocs` with the following command. The documentation can then be accessed with the browser under `http://localhost:8000`.

```console
make documentation
```
